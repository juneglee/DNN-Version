{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 64\n",
    "dropout_rate = 0.1\n",
    "class_n = len(train_total['disease_code'].unique())\n",
    "learning_rate = 1e-4\n",
    "epochs = 50\n",
    "save_path = 'models/base.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# albumentations_transform = A.Compose([\n",
    "#     A.OneOf([\n",
    "#                           A.HorizontalFlip(p=1),\n",
    "#                           A.RandomRotate90(p=1),\n",
    "#                           A.VerticalFlip(p=1)            \n",
    "#     ], p=1),\n",
    "#     A.OneOf([\n",
    "#                           A.MotionBlur(p=1),\n",
    "#                           A.OpticalDistortion(p=1),\n",
    "#                           A.GaussNoise(p=1)                 \n",
    "#     ], p=1),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, labels=None, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        if mode == 'train':\n",
    "            self.labels = labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if self.mode == 'train':\n",
    "            img = cv2.imread('data/train_imgs/'+self.files[i])\n",
    "            img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
    "            img = img.astype(np.float32)/255\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.labels[i], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            img = cv2.imread('data/test_imgs/'+self.files[i])\n",
    "            img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
    "            img = img.astype(np.float32)/255\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_total.iloc[:200]\n",
    "val = train_total.iloc[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['img_path'].str.split('/').str[-1].values, train['disease_code'].values)\n",
    "val_dataset = CustomDataset(val['img_path'].str.split('/').str[-1].values, val['disease_code'].values)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "test_dataset = CustomDataset(test['img_path'].str.split('/').str[-1], labels=None, mode='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, class_n, rate=0.1):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.output_layer = nn.Linear(in_features=1000, out_features=class_n, bias=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.output_layer(self.dropout(self.model(inputs)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model(class_n).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    img = batch_item['img'].to(device)\n",
    "    label = batch_item['label'].to(device)\n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:15,  3.88s/it, Epoch=1, Loss=0.999866, Total Loss=1.308564]\n",
      "1it [00:04,  4.08s/it, Epoch=1, Val Loss=0.944481, Total Val Loss=0.944481]\n",
      "4it [00:15,  3.90s/it, Epoch=2, Loss=0.424079, Total Loss=0.157262]\n",
      "1it [00:04,  4.06s/it, Epoch=2, Val Loss=0.688083, Total Val Loss=0.688083]\n",
      "4it [00:15,  3.95s/it, Epoch=3, Loss=0.121874, Total Loss=0.054790]\n",
      "1it [00:04,  4.07s/it, Epoch=3, Val Loss=0.500526, Total Val Loss=0.500526]\n",
      "4it [00:15,  3.88s/it, Epoch=4, Loss=0.219723, Total Loss=0.067268]\n",
      "1it [00:04,  4.10s/it, Epoch=4, Val Loss=0.459876, Total Val Loss=0.459876]\n",
      "4it [00:15,  3.89s/it, Epoch=5, Loss=0.098606, Total Loss=0.036424]\n",
      "1it [00:04,  4.16s/it, Epoch=5, Val Loss=0.514150, Total Val Loss=0.514150]\n",
      "4it [00:15,  3.88s/it, Epoch=6, Loss=0.452386, Total Loss=0.148932]\n",
      "1it [00:04,  4.04s/it, Epoch=6, Val Loss=0.535790, Total Val Loss=0.535790]\n",
      "4it [00:15,  3.88s/it, Epoch=7, Loss=0.086329, Total Loss=0.025206]\n",
      "1it [00:04,  4.10s/it, Epoch=7, Val Loss=0.659844, Total Val Loss=0.659844]\n",
      "4it [00:15,  3.86s/it, Epoch=8, Loss=0.034602, Total Loss=0.013212]\n",
      "1it [00:04,  4.14s/it, Epoch=8, Val Loss=0.658737, Total Val Loss=0.658737]\n",
      "4it [00:15,  3.89s/it, Epoch=9, Loss=0.005305, Total Loss=0.005886]\n",
      "1it [00:04,  4.19s/it, Epoch=9, Val Loss=0.602692, Total Val Loss=0.602692]\n",
      "4it [00:15,  3.90s/it, Epoch=10, Loss=0.288429, Total Loss=0.080576]\n",
      "1it [00:04,  4.10s/it, Epoch=10, Val Loss=0.517629, Total Val Loss=0.517629]\n",
      "4it [00:15,  3.87s/it, Epoch=11, Loss=0.128380, Total Loss=0.036900]\n",
      "1it [00:04,  4.11s/it, Epoch=11, Val Loss=0.308710, Total Val Loss=0.308710]\n",
      "4it [00:15,  3.91s/it, Epoch=12, Loss=0.006502, Total Loss=0.004492]\n",
      "1it [00:04,  4.09s/it, Epoch=12, Val Loss=0.163772, Total Val Loss=0.163772]\n",
      "4it [00:15,  3.86s/it, Epoch=13, Loss=0.062863, Total Loss=0.032429]\n",
      "1it [00:04,  4.09s/it, Epoch=13, Val Loss=0.131557, Total Val Loss=0.131557]\n",
      "4it [00:15,  3.91s/it, Epoch=14, Loss=0.017981, Total Loss=0.009424]\n",
      "1it [00:04,  4.09s/it, Epoch=14, Val Loss=0.120165, Total Val Loss=0.120165]\n",
      "4it [00:15,  3.94s/it, Epoch=15, Loss=0.054553, Total Loss=0.022629]\n",
      "1it [00:04,  4.08s/it, Epoch=15, Val Loss=0.132862, Total Val Loss=0.132862]\n",
      "4it [00:15,  3.88s/it, Epoch=16, Loss=0.696704, Total Loss=0.175403]\n",
      "1it [00:04,  4.11s/it, Epoch=16, Val Loss=0.252227, Total Val Loss=0.252227]\n",
      "4it [00:15,  3.91s/it, Epoch=17, Loss=0.438943, Total Loss=0.112482]\n",
      "1it [00:04,  4.07s/it, Epoch=17, Val Loss=0.321450, Total Val Loss=0.321450]\n",
      "4it [00:15,  3.87s/it, Epoch=18, Loss=0.342314, Total Loss=0.115889]\n",
      "1it [00:04,  4.08s/it, Epoch=18, Val Loss=0.668900, Total Val Loss=0.668900]\n",
      "4it [00:15,  3.92s/it, Epoch=19, Loss=0.093714, Total Loss=0.090354]\n",
      "1it [00:04,  4.15s/it, Epoch=19, Val Loss=0.651414, Total Val Loss=0.651414]\n",
      "4it [00:15,  3.88s/it, Epoch=20, Loss=1.033284, Total Loss=0.285024]\n",
      "1it [00:04,  4.15s/it, Epoch=20, Val Loss=0.317319, Total Val Loss=0.317319]\n",
      "4it [00:15,  3.91s/it, Epoch=21, Loss=0.667133, Total Loss=0.183326]\n",
      "1it [00:04,  4.11s/it, Epoch=21, Val Loss=0.283383, Total Val Loss=0.283383]\n",
      "4it [00:15,  3.90s/it, Epoch=22, Loss=0.001903, Total Loss=0.006690]\n",
      "1it [00:04,  4.08s/it, Epoch=22, Val Loss=0.546278, Total Val Loss=0.546278]\n",
      "4it [00:15,  3.87s/it, Epoch=23, Loss=1.297329, Total Loss=0.388121]\n",
      "1it [00:04,  4.07s/it, Epoch=23, Val Loss=0.595647, Total Val Loss=0.595647]\n",
      "4it [00:15,  3.90s/it, Epoch=24, Loss=0.301001, Total Loss=0.132911]\n",
      "1it [00:04,  4.04s/it, Epoch=24, Val Loss=0.437978, Total Val Loss=0.437978]\n",
      "4it [00:15,  3.85s/it, Epoch=25, Loss=0.283524, Total Loss=0.116130]\n",
      "1it [00:04,  4.07s/it, Epoch=25, Val Loss=0.544773, Total Val Loss=0.544773]\n",
      "4it [00:15,  3.89s/it, Epoch=26, Loss=0.166631, Total Loss=0.088370]\n",
      "1it [00:04,  4.08s/it, Epoch=26, Val Loss=0.256078, Total Val Loss=0.256078]\n",
      "4it [00:15,  3.89s/it, Epoch=27, Loss=0.027110, Total Loss=0.016507]\n",
      "1it [00:04,  4.15s/it, Epoch=27, Val Loss=0.210046, Total Val Loss=0.210046]\n",
      "4it [00:15,  3.87s/it, Epoch=28, Loss=0.044750, Total Loss=0.014226]\n",
      "1it [00:04,  4.17s/it, Epoch=28, Val Loss=0.265348, Total Val Loss=0.265348]\n",
      "4it [00:15,  3.87s/it, Epoch=29, Loss=0.003811, Total Loss=0.003205]\n",
      "1it [00:04,  4.10s/it, Epoch=29, Val Loss=0.301636, Total Val Loss=0.301636]\n",
      "4it [00:15,  3.85s/it, Epoch=30, Loss=0.002099, Total Loss=0.003328]\n",
      "1it [00:04,  4.12s/it, Epoch=30, Val Loss=0.338837, Total Val Loss=0.338837]\n",
      "4it [00:15,  3.84s/it, Epoch=31, Loss=0.130637, Total Loss=0.037582]\n",
      "1it [00:04,  4.10s/it, Epoch=31, Val Loss=0.358804, Total Val Loss=0.358804]\n",
      "4it [00:15,  3.89s/it, Epoch=32, Loss=0.041996, Total Loss=0.011771]\n",
      "1it [00:04,  4.14s/it, Epoch=32, Val Loss=0.489831, Total Val Loss=0.489831]\n",
      "4it [00:15,  3.88s/it, Epoch=33, Loss=0.017935, Total Loss=0.009566]\n",
      "1it [00:04,  4.10s/it, Epoch=33, Val Loss=0.570777, Total Val Loss=0.570777]\n",
      "4it [00:15,  3.87s/it, Epoch=34, Loss=1.108376, Total Loss=0.278529]\n",
      "1it [00:04,  4.10s/it, Epoch=34, Val Loss=0.628970, Total Val Loss=0.628970]\n",
      "4it [00:15,  3.87s/it, Epoch=35, Loss=0.001259, Total Loss=0.001299]\n",
      "1it [00:04,  4.04s/it, Epoch=35, Val Loss=0.562285, Total Val Loss=0.562285]\n",
      "4it [00:15,  3.90s/it, Epoch=36, Loss=0.006141, Total Loss=0.002985]\n",
      "1it [00:04,  4.05s/it, Epoch=36, Val Loss=0.521478, Total Val Loss=0.521478]\n",
      "4it [00:15,  3.84s/it, Epoch=37, Loss=0.000073, Total Loss=0.001649]\n",
      "1it [00:04,  4.06s/it, Epoch=37, Val Loss=0.502390, Total Val Loss=0.502390]\n",
      "4it [00:15,  3.85s/it, Epoch=38, Loss=0.004531, Total Loss=0.004110]\n",
      "1it [00:04,  4.09s/it, Epoch=38, Val Loss=0.467585, Total Val Loss=0.467585]\n",
      "4it [00:15,  3.85s/it, Epoch=39, Loss=0.135309, Total Loss=0.037608]\n",
      "1it [00:04,  4.08s/it, Epoch=39, Val Loss=0.551944, Total Val Loss=0.551944]\n",
      "4it [00:15,  3.87s/it, Epoch=40, Loss=0.116291, Total Loss=0.032304]\n",
      "1it [00:04,  4.06s/it, Epoch=40, Val Loss=0.601930, Total Val Loss=0.601930]\n",
      "4it [00:15,  3.87s/it, Epoch=41, Loss=0.665286, Total Loss=0.168873]\n",
      "1it [00:04,  4.06s/it, Epoch=41, Val Loss=0.534466, Total Val Loss=0.534466]\n",
      "4it [00:15,  3.86s/it, Epoch=42, Loss=0.313636, Total Loss=0.083186]\n",
      "1it [00:04,  4.10s/it, Epoch=42, Val Loss=0.474925, Total Val Loss=0.474925]\n",
      "4it [00:15,  3.84s/it, Epoch=43, Loss=0.092646, Total Loss=0.039811]\n",
      "1it [00:04,  4.07s/it, Epoch=43, Val Loss=0.566959, Total Val Loss=0.566959]\n",
      "4it [00:15,  3.86s/it, Epoch=44, Loss=0.398070, Total Loss=0.148375]\n",
      "1it [00:04,  4.08s/it, Epoch=44, Val Loss=0.557170, Total Val Loss=0.557170]\n",
      "4it [00:15,  3.92s/it, Epoch=45, Loss=0.019993, Total Loss=0.027688]\n",
      "1it [00:04,  4.14s/it, Epoch=45, Val Loss=0.626512, Total Val Loss=0.626512]\n",
      "4it [00:15,  3.95s/it, Epoch=46, Loss=0.000530, Total Loss=0.014943]\n",
      "1it [00:04,  4.23s/it, Epoch=46, Val Loss=0.640151, Total Val Loss=0.640151]\n",
      "4it [00:15,  3.87s/it, Epoch=47, Loss=0.246003, Total Loss=0.066428]\n",
      "1it [00:04,  4.11s/it, Epoch=47, Val Loss=0.567608, Total Val Loss=0.567608]\n",
      "4it [00:15,  3.85s/it, Epoch=48, Loss=0.004169, Total Loss=0.009660]\n",
      "1it [00:04,  4.06s/it, Epoch=48, Val Loss=0.682849, Total Val Loss=0.682849]\n",
      "4it [00:15,  3.83s/it, Epoch=49, Loss=0.053044, Total Loss=0.017917]\n",
      "1it [00:04,  4.09s/it, Epoch=49, Val Loss=0.683520, Total Val Loss=0.683520]\n",
      "4it [00:15,  3.84s/it, Epoch=50, Loss=0.215748, Total Loss=0.057899]\n",
      "1it [00:04,  4.07s/it, Epoch=50, Val Loss=0.683226, Total Val Loss=0.683226]\n"
     ]
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "    \n",
    "    if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "        torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):\n",
    "    model.eval()\n",
    "    tqdm_dataset = tqdm(enumerate(dataset))\n",
    "    training = False\n",
    "    results = []\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        img = batch_item['img'].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "        output = torch.tensor(torch.argmax(output, axis=-1), dtype=torch.int32).cpu().numpy()\n",
    "        results.extend(output)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "75it [05:31,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(test_dataloader)\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission.iloc[:,1] = preds\n",
    "submission.to_csv('baseline.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
